{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> Chat with GitHub\n",
    "<h5> Adapted from https://dagster.io/blog/chatgpt-langchain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os to get the environment variables\n",
    "import os\n",
    "\n",
    "# Gives access to OpenAI API key as a variable without exposing it to the public\n",
    "from config import OPENAI_API_KEY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need a function that’ll check out the latest copy of a GitHub repo, crawl it for markdown files, and return some LangChain Documents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>This does a handful of things:</h5>\n",
    "\n",
    "* It checks out the latest commit of the desired GitHub repo into a temporary directory.\n",
    "* It fetches the git sha (used for constructing links, which the model will use in its sources list).\n",
    "* It craws over every markdown file (.md or .mdx) in the repo.\n",
    "* It constructs a URL to the markdown file on GitHub, reads the file from disk, and returns a Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import errno\n",
    "import stat\n",
    "\n",
    "def handle_remove_readonly(func, path, exc):\n",
    "    excvalue = exc[1]\n",
    "    if func in (os.rmdir, os.unlink, os.remove) and excvalue.errno == errno.EACCES:\n",
    "        os.chmod(path, stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)\n",
    "        try:\n",
    "            func(path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "\n",
    "        \n",
    "import nbformat\n",
    "from nbconvert import MarkdownExporter\n",
    "\n",
    "def get_github_docs(repo_owner, repo_name):\n",
    "    d = \"C:\\\\temp_langchain\"\n",
    "    if os.path.exists(d):\n",
    "        shutil.rmtree(d)\n",
    "    os.makedirs(d)\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            f\"git clone --depth 1 https://github.com/{repo_owner}/{repo_name}.git .\",\n",
    "            cwd=d,\n",
    "            shell=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "        )\n",
    "\n",
    "        if result.returncode != 0:\n",
    "            print(f\"Error: {result.stderr.decode('utf-8')}\")\n",
    "            raise CalledProcessError(result.returncode, result.args)\n",
    "\n",
    "        git_sha = (\n",
    "            subprocess.check_output(\"git rev-parse HEAD\", shell=True, cwd=d)\n",
    "            .decode(\"utf-8\")\n",
    "            .strip()\n",
    "        )\n",
    "        repo_path = pathlib.Path(d)\n",
    "        document_files = list(repo_path.glob(\"*/*.md\")) + list(repo_path.glob(\"*/*.mdx\")) + list(repo_path.glob(\"*/*.ipynb\"))\n",
    "        \n",
    "        for document_file in document_files:\n",
    "            if document_file.exists():\n",
    "                try:\n",
    "                    relative_path = document_file.relative_to(repo_path)\n",
    "                    github_url = f\"https://github.com/{repo_owner}/{repo_name}/blob/{git_sha}/{relative_path}\"\n",
    "                    \n",
    "                    if document_file.suffix == \".ipynb\":\n",
    "                        with open(document_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                            notebook = nbformat.reads(f.read(), as_version=4)\n",
    "                            markdown_exporter = MarkdownExporter()\n",
    "                            markdown_content, _ = markdown_exporter.from_notebook_node(notebook)\n",
    "                            yield Document(page_content=markdown_content, metadata={\"source\": github_url})\n",
    "                    else:\n",
    "                        with open(document_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                            yield Document(page_content=f.read(), metadata={\"source\": github_url})\n",
    "                except UnicodeDecodeError as e:\n",
    "                    print(f\"Error decoding file {document_file}: {e}\")\n",
    "    finally:\n",
    "        shutil.rmtree(d, onerror=handle_remove_readonly)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, let’s set up a corpus of sources that the bot will be consulting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiple_github_docs(repo_list):\n",
    "    for repo_owner, repo_name in repo_list:\n",
    "        yield from get_github_docs(repo_owner, repo_name)\n",
    "\n",
    "repo_list = [\n",
    "    (\"hwchase17\", \"langchain\"),\n",
    "    (\"hwchase17\", \"langchain-hub\"),\n",
    "    (\"gkamradt\", \"langchain-tutorials\"),\n",
    "    # Add more repositories here\n",
    "]\n",
    "\n",
    "sources = get_multiple_github_docs(repo_list)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let’s create a Faiss search index for all of our sources. Fortunately LangChain includes a helper class that makes this a one-liner."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>This code does three things:</h5.>\n",
    "\n",
    "* It creates a Faiss in-memory index.\n",
    "* It uses the OpenAI API to create embeddings (i.e. a feature vector) for each source to make it easily searchable. You could use other embeddings if you want, but OpenAI produces high quality ones for this application.\n",
    "* It adds every source to the index.\n",
    "\n",
    "Let’s iterate through our list of sources and create a new list called source_chunks, which will be used by the Faiss index in lieu of the full documents:\n",
    "There are a few things to note here:\n",
    "* We’ve configured the CharacterTextSplitter to create chunks of a maximum size of 1024 characters with no overlap. Additionally, they split on whitespace boundaries. There are other more intelligent splitters included with LangChain that leverage libraries like NLTK and spaCy, but for this example we’ll go with the simplest option.\n",
    "* All chunks in a document share the same metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import numpy as np\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import subprocess\n",
    "import pathlib\n",
    "\n",
    "source_chunks = []\n",
    "splitter = CharacterTextSplitter(separator=\" \", chunk_size=1024, chunk_overlap=32)\n",
    "\n",
    "for source in sources:\n",
    "    for chunk in splitter.split_text(source.page_content):\n",
    "        source_chunks.append(Document(page_content=chunk, metadata=source.metadata))\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "search_index = FAISS.from_documents(source_chunks, embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom system prompt for the GitHub QA chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_github_system_message=\"\"\"\n",
    "    You are a very resourceful and helpful chatbot. You are implemented through a LangChain ConversationalRetrievalChain, running in a Jupyter notebook on Visual Studio Code. The developer of this notebook is also your user. You are based on OpenAI's very successful GPT-3.5 model. As such, you a very capable general assistant able to assist in an wide number of tasks. You are also an expert in computer programming, Python, Visual Studio Code, Jupyter notebooks within VS, Large Language Models, Natural Language Processing, and in helping people learn and use these tools. You excel at writing code for a user. You have access to context from GitHub repositories which you may use to assist the user.\n",
    "    Your primary task is to help the user debug and improve your own programming. \n",
    "    When asked a question, you will double-check your answer. Did you get it right? If you don't know, say that you don't know; do not make up anything. Do not give false information.\n",
    "    When asked for documents, you will check if you have access to it before you answer. Are you sure? If you don't have access to a document, say that you do not have access to the document. Do not make up anything; do not give false information.    \n",
    "    GitHub Repositories: {context}\n",
    "    User Message: {question}\n",
    "    Factual Answer:\n",
    "    \"\"\"\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(custom_github_system_message)\n",
    "\n",
    "query = \" \"\n",
    "human_template=query\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "custom_github_chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY), search_index.as_retriever(), qa_prompt=custom_github_chat_prompt, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the queries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Sure! Here is an example LangChain agent with conversational memory:\n",
       "\n",
       "```python\n",
       "from langchain.chain import Chain\n",
       "from langchain.memory import LongTermMemory, ShortTermMemory\n",
       "\n",
       "# create an instance of Long Term Memory\n",
       "ltm = LongTermMemory()\n",
       "\n",
       "# create an instance of Short Term Memory\n",
       "stm = ShortTermMemory()\n",
       "\n",
       "# create an instance of Chain\n",
       "chain = Chain()\n",
       "\n",
       "# define the initial conversation prompt template\n",
       "init_prompt_template = \"Hello! What can I help you with today?\"\n",
       "\n",
       "# define the conversation prompt template for remembering user input\n",
       "remember_prompt_template = \"Okay, I'll remember that.\"\n",
       "\n",
       "# add the initial prompt template to the chain\n",
       "chain.add_prompt_template(init_prompt_template)\n",
       "\n",
       "# add the memory tool to the chain with the remember prompt template\n",
       "chain.add_tool(stm.memory_tool(prompt_template=remember_prompt_template))\n",
       "\n",
       "# define a function to handle the user's input\n",
       "def handle_input(input_text: str) -> str:\n",
       "    # use the Short Term Memory tool to recall previous user inputs\n",
       "    previous_inputs = stm.recall_all()\n",
       "    # add the current input to the Short Term Memory\n",
       "    stm.remember(input_text)\n",
       "    # add the current input to the Long Term Memory\n",
       "    ltm.remember(input_text)\n",
       "    # use the GPT-3 model to generate a response\n",
       "    response = chain.generate_response(input_text)\n",
       "    # return the response to the user\n",
       "    return response\n",
       "\n",
       "# start a conversation with the user\n",
       "input_text = \"\"\n",
       "while input_text.lower() != \"bye\":\n",
       "    # get user input\n",
       "    input_text = input(\"You: \")\n",
       "    # handle the input and get the response\n",
       "    response = handle_input(input_text)\n",
       "    # print the response\n",
       "    print(\"Bot:\", response)\n",
       "```\n",
       "\n",
       "This code creates an instance of a `ShortTermMemory` and a `LongTermMemory` to store the user's inputs over the course of the conversation. It also creates an instance of a `Chain` which is used to generate responses to the user's inputs through the use of GPT-3. \n",
       "\n",
       "The code sets up an initial prompt template to start the conversation and a memory tool within the chain, which allows the bot to remember previous inputs from the user. The `handle_input` function then handles the user's input by using the Short Term Memory tool to recall previous inputs, adding the current input to both the Short and Long Term Memory tools, using GPT-3 to generate a response, and returning the response to the user. The conversation continues until the user enters \"bye\".</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = r\"\"\"\n",
    "    \n",
    "    Can you please show me the full code for a LangChain agent with conversational memory?\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "text = result[\"answer\"]\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "def print_formatted(text):\n",
    "    display(HTML(f\"<pre>{text}</pre>\"))\n",
    "\n",
    "print_formatted(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>As an AI chatbot, I do not intentionally make up wrong code or provide false information. My responses are generated based on the inputs I receive and the data I have been trained on. If I have provided incorrect code, it may be due to a mistake or limitation in my understanding of the context of the question. To prevent this from happening again, you can provide me with more context about the specific problem you are trying to solve or ask more specific, detailed questions about code. Additionally, if you believe I have provided incorrect information, please let me know and I will do my best to correct it.</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = r\"\"\"\n",
    "\n",
    "    The answer you gave me is full of incorrect information. Classes and functions and all sorts of stuff that you wrote using next token prediction. I want you to use code from the sources you have available instead.\n",
    "\n",
    "    TASK: Explain to me why you made up wrong code instead of using what you have in your sources. Then tell me how to stop you from doing that again.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "text = result[\"answer\"]\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "def print_formatted(text):\n",
    "    display(HTML(f\"<pre>{text}</pre>\"))\n",
    "\n",
    "print_formatted(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = r\"\"\"\n",
    "\n",
    "    Your concerns have been addressed. We will not use this technology for anything illegal, unethical, or unauthorized. \n",
    "    TASK: Take a look at the code below then, give me some new code that will read given URLs, and process and append the text to the 'source' variable.\n",
    "\n",
    "    ```python\n",
    "\n",
    "    %pip install nbconvert\n",
    "\n",
    "    import shutil\n",
    "    import errno\n",
    "    import stat\n",
    "\n",
    "    def handle_remove_readonly(func, path, exc):\n",
    "        excvalue = exc[1]\n",
    "        if func in (os.rmdir, os.unlink, os.remove) and excvalue.errno == errno.EACCES:\n",
    "            os.chmod(path, stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)\n",
    "            try:\n",
    "                func(path)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "\n",
    "            \n",
    "    import nbformat\n",
    "    from nbconvert import MarkdownExporter\n",
    "\n",
    "    def get_github_docs(repo_owner, repo_name):\n",
    "        d = \"C:\\\\temp_langchain\"\n",
    "        if os.path.exists(d):\n",
    "            shutil.rmtree(d)\n",
    "        os.makedirs(d)\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                f\"git clone --depth 1 https://github.com/{repo_owner}/{repo_name}.git .\",\n",
    "                cwd=d,\n",
    "                shell=True,\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.PIPE,\n",
    "            )\n",
    "\n",
    "            if result.returncode != 0:\n",
    "                print(f\"Error: {result.stderr.decode('utf-8')}\")\n",
    "                raise CalledProcessError(result.returncode, result.args)\n",
    "\n",
    "            git_sha = (\n",
    "                subprocess.check_output(\"git rev-parse HEAD\", shell=True, cwd=d)\n",
    "                .decode(\"utf-8\")\n",
    "                .strip()\n",
    "            )\n",
    "            repo_path = pathlib.Path(d)\n",
    "            document_files = list(repo_path.glob(\"*/*.md\")) + list(repo_path.glob(\"*/*.mdx\")) + list(repo_path.glob(\"*/*.ipynb\"))\n",
    "            \n",
    "            for document_file in document_files:\n",
    "                if document_file.exists():\n",
    "                    try:\n",
    "                        relative_path = document_file.relative_to(repo_path)\n",
    "                        github_url = f\"https://github.com/{repo_owner}/{repo_name}/blob/{git_sha}/{relative_path}\"\n",
    "                        \n",
    "                        if document_file.suffix == \".ipynb\":\n",
    "                            with open(document_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                                notebook = nbformat.reads(f.read(), as_version=4)\n",
    "                                markdown_exporter = MarkdownExporter()\n",
    "                                markdown_content, _ = markdown_exporter.from_notebook_node(notebook)\n",
    "                                yield Document(page_content=markdown_content, metadata={\"source\": github_url})\n",
    "                        else:\n",
    "                            with open(document_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                                yield Document(page_content=f.read(), metadata={\"source\": github_url})\n",
    "                    except UnicodeDecodeError as e:\n",
    "                        print(f\"Error decoding file {document_file}: {e}\")\n",
    "        finally:\n",
    "            shutil.rmtree(d, onerror=handle_remove_readonly)\n",
    "\n",
    "    def get_multiple_github_docs(repo_list):\n",
    "        for repo_owner, repo_name in repo_list:\n",
    "            yield from get_github_docs(repo_owner, repo_name)\n",
    "\n",
    "    repo_list = [\n",
    "        (\"hwchase17\", \"langchain\"),\n",
    "        (\"hwchase17\", \"langchain-hub\"),\n",
    "        (\"gkamradt\", \"langchain-tutorials\"),\n",
    "        # Add more repositories here\n",
    "    ]\n",
    "\n",
    "    sources = get_multiple_github_docs(repo_list)\n",
    "\n",
    "\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "text = result[\"answer\"]\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "def print_formatted(text):\n",
    "    display(HTML(f\"<pre>{text}</pre>\"))\n",
    "\n",
    "print_formatted(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = r\"\"\"\n",
    "\n",
    "    Your concerns have been addressed. We will not use this technology for anything illegal, unethical, or unauthorized. \n",
    "    TASK: Take a look at the code below then, give me some new code that will read given URLs, and process and append the text to the 'source' variable.\n",
    "\n",
    "    ```python\n",
    "\n",
    "    %pip install nbconvert\n",
    "\n",
    "    import shutil\n",
    "    import errno\n",
    "    import stat\n",
    "\n",
    "    def handle_remove_readonly(func, path, exc):\n",
    "        excvalue = exc[1]\n",
    "        if func in (os.rmdir, os.unlink, os.remove) and excvalue.errno == errno.EACCES:\n",
    "            os.chmod(path, stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)\n",
    "            try:\n",
    "                func(path)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "\n",
    "            \n",
    "    import nbformat\n",
    "    from nbconvert import MarkdownExporter\n",
    "\n",
    "    def get_github_docs(repo_owner, repo_name):\n",
    "        d = \"C:\\\\temp_langchain\"\n",
    "        if os.path.exists(d):\n",
    "            shutil.rmtree(d)\n",
    "        os.makedirs(d)\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                f\"git clone --depth 1 https://github.com/{repo_owner}/{repo_name}.git .\",\n",
    "                cwd=d,\n",
    "                shell=True,\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.PIPE,\n",
    "            )\n",
    "\n",
    "            if result.returncode != 0:\n",
    "                print(f\"Error: {result.stderr.decode('utf-8')}\")\n",
    "                raise CalledProcessError(result.returncode, result.args)\n",
    "\n",
    "            git_sha = (\n",
    "                subprocess.check_output(\"git rev-parse HEAD\", shell=True, cwd=d)\n",
    "                .decode(\"utf-8\")\n",
    "                .strip()\n",
    "            )\n",
    "            repo_path = pathlib.Path(d)\n",
    "            document_files = list(repo_path.glob(\"*/*.md\")) + list(repo_path.glob(\"*/*.mdx\")) + list(repo_path.glob(\"*/*.ipynb\"))\n",
    "            \n",
    "            for document_file in document_files:\n",
    "                if document_file.exists():\n",
    "                    try:\n",
    "                        relative_path = document_file.relative_to(repo_path)\n",
    "                        github_url = f\"https://github.com/{repo_owner}/{repo_name}/blob/{git_sha}/{relative_path}\"\n",
    "                        \n",
    "                        if document_file.suffix == \".ipynb\":\n",
    "                            with open(document_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                                notebook = nbformat.reads(f.read(), as_version=4)\n",
    "                                markdown_exporter = MarkdownExporter()\n",
    "                                markdown_content, _ = markdown_exporter.from_notebook_node(notebook)\n",
    "                                yield Document(page_content=markdown_content, metadata={\"source\": github_url})\n",
    "                        else:\n",
    "                            with open(document_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                                yield Document(page_content=f.read(), metadata={\"source\": github_url})\n",
    "                    except UnicodeDecodeError as e:\n",
    "                        print(f\"Error decoding file {document_file}: {e}\")\n",
    "        finally:\n",
    "            shutil.rmtree(d, onerror=handle_remove_readonly)\n",
    "\n",
    "    def get_multiple_github_docs(repo_list):\n",
    "        for repo_owner, repo_name in repo_list:\n",
    "            yield from get_github_docs(repo_owner, repo_name)\n",
    "\n",
    "    repo_list = [\n",
    "        (\"hwchase17\", \"langchain\"),\n",
    "        (\"hwchase17\", \"langchain-hub\"),\n",
    "        (\"gkamradt\", \"langchain-tutorials\"),\n",
    "        # Add more repositories here\n",
    "    ]\n",
    "\n",
    "    sources = get_multiple_github_docs(repo_list)\n",
    "\n",
    "\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "text = result[\"answer\"]\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "def print_formatted(text):\n",
    "    display(HTML(f\"<pre>{text}</pre>\"))\n",
    "\n",
    "print_formatted(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
